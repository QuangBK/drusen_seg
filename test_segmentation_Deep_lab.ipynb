{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import pickle\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unet import UNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose,\n",
    "    RandomCrop, Normalize, Resize\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DeepLabv3 Model download and change the head for your prediction\"\"\"\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "def createDeepLabv3(outputchannels=1):\n",
    "    model = models.segmentation.deeplabv3_resnet101(\n",
    "        pretrained=True, progress=True)\n",
    "    # Added a Sigmoid activation after the last convolution layer\n",
    "    model.classifier = DeepLabHead(2048, outputchannels)\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 512, 512\n",
    "batchSize = 4\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAug:\n",
    "    def __init__(self, aug):\n",
    "        self.aug=aug\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.aug(image=img)['image']\n",
    "        return img\n",
    "\n",
    "class FundusDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, PATH_IMG, transform=None, transform_torch=None, toTensor=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.PATH_IMG = PATH_IMG\n",
    "        self.ToTensor = toTensor\n",
    "        self.transform = transform\n",
    "        self.transform_torch = transform_torch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PATH_IMG)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name_temp = self.PATH_IMG[idx].split('/')[-1]\n",
    "        image = cv2.imread(self.PATH_IMG[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img_auged = self.transform(image=image)\n",
    "            image = img_auged['image']\n",
    "            image = self.transform_torch(image)\n",
    "        sample = {'image': image, 'id': file_name_temp}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    \n",
    "train_transform_torch = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "test_album = Compose([\n",
    "    Resize(IMG_HEIGHT, IMG_WIDTH)\n",
    "])\n",
    "\n",
    "test_transform_torch = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = createDeepLabv3()\n",
    "model = nn.DataParallel(createDeepLabv3()).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "### Note: you should change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('./saved_model/image_DeepLab/net_040.pth'))\n",
    "model.load_state_dict(torch.load('/home/quang/working/fundus_segmentation/results/image_DeepLab/net_040.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate mask for all data\n",
    "### Note: you should change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '/home/quang/working/Face-Aging-CAAE/data/data_amd_resize_1200_8196/'\n",
    "PATH_OUTPUT = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/prediction_data_amd_512_8196/'\n",
    "\n",
    "list_paths_img = glob.glob(PATH_DATA + '*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 257/257 [06:44<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset_all = FundusDataset(list_paths_img, transform=test_album, transform_torch=test_transform_torch, \n",
    "                              toTensor=transforms.ToTensor())\n",
    "\n",
    "dataloader_valid_all = torch.utils.data.DataLoader(valid_dataset_all,\n",
    "                                             batch_size=32, shuffle=False,\n",
    "                                             num_workers=2)\n",
    "\n",
    "# requires_grad(net, False)\n",
    "model.eval()\n",
    "dice_list = []\n",
    "TN = 0\n",
    "FP = 0\n",
    "TP = 0\n",
    "FN = 0\n",
    "sensivity_list = []\n",
    "specitivity_list = []\n",
    "for j_,sample in enumerate(tqdm(dataloader_valid_all)):\n",
    "\n",
    "    inputs = sample['image'].to(device)\n",
    "    ids_list = sample['id']\n",
    "    # zero the parameter gradients\n",
    "\n",
    "    # track history if only in train\n",
    "    with torch.set_grad_enabled(False):\n",
    "        outputs = model(inputs)\n",
    "        y_pred = torch.sigmoid(outputs['out'])\n",
    "        y_pred = y_pred.data.cpu().numpy()\n",
    "\n",
    "        for idx_temp, f_name_temp in enumerate(ids_list):\n",
    "            cv2.imwrite(PATH_OUTPUT + f_name_temp.split('/')[-1] ,(y_pred[idx_temp][0]*255).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
