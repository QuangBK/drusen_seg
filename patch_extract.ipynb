{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "INPUT_DIR = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full/'\n",
    "INPUT_DIR_MASK = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test\n",
    "list_names = glob.glob(INPUT_DIR + '*.jpg')\n",
    "list_name_f = [x.split('/')[-1].split('.')[0] for x in list_names]\n",
    "print (len(list_names))\n",
    "\n",
    "list_name_f_train, list_name_f_test = train_test_split(list_name_f, test_size=0.15, random_state=42)\n",
    "print (len(list_name_f_train), len(list_name_f_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop images and mask for training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 1200\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIR_PATCH_MASK_PRED = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full_mask_pred_DeepLabV3_ver2/'\n",
    "OUTPUT_DIR = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/patch_data_train_DeepLabV3_ver2/'\n",
    "OUTPUT_MAKS_DIR = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/patch_data_mask_pred_train_DeepLabV3_ver2/'\n",
    "\n",
    "for file_name_temp in tqdm(list_name_f_train):\n",
    "    img = cv2.imread(INPUT_DIR + file_name_temp + '.jpg')\n",
    "    mask = cv2.imread(INPUT_DIR_MASK + file_name_temp + '.labels.tif', 0)\n",
    "    \n",
    "    mask_pred = cv2.imread(INPUT_DIR_PATCH_MASK_PRED + file_name_temp + '.jpg', 0)\n",
    "\n",
    "    img_cropped = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    mask = np.where(mask > 0, 255, 0).astype(np.uint8)\n",
    "    mask_cropped = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    mask_cropped = np.where(mask_cropped > 128, 255, 0).astype(np.uint8)\n",
    "\n",
    "    mask_pred_cropped = cv2.resize(mask_pred, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "    img_gray_mask = np.where(img_gray > 10, 1, 0)\n",
    "\n",
    "    circle = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "    cv2.circle(circle, (int(IMG_SIZE/2), int(IMG_SIZE/2)), int(IMG_SIZE/2), 1, thickness=-1)\n",
    "\n",
    "    img_gray_mask = img_gray_mask*circle\n",
    "\n",
    "    temp = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "    for i in range(0, IMG_SIZE-PATCH_SIZE, 100):\n",
    "        for j in range(0, IMG_SIZE-PATCH_SIZE, 100):\n",
    "            if img_gray_mask[i:i+PATCH_SIZE, j:j+PATCH_SIZE].sum() > PATCH_SIZE*PATCH_SIZE/2 and mask_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE].sum() > 0:\n",
    "                img_patch = img_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE,:]\n",
    "                mask_patch = mask_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
    "                mask_pred_cropped_patch = mask_pred_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
    "                cv2.imwrite(OUTPUT_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.jpg',img_patch)\n",
    "                cv2.imwrite(OUTPUT_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.labels.tif',mask_patch)\n",
    "                cv2.imwrite(OUTPUT_MAKS_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.jpg',mask_pred_cropped_patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop images and mask for testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:10<00:00,  7.36it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 1200\n",
    "PATCH_SIZE = 128\n",
    "\n",
    "INPUT_DIR_PATCH_MASK_PRED = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full_mask_pred_DeepLabV3_ver2/'\n",
    "OUTPUT_DIR = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/patch_data_test_DeepLabV3_ver2/'\n",
    "OUTPUT_MAKS_DIR = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/patch_data_mask_pred_test_DeepLabV3_ver2/'\n",
    "\n",
    "for file_name_temp in tqdm(list_name_f_test):\n",
    "    img = cv2.imread(INPUT_DIR + file_name_temp + '.jpg')\n",
    "    mask = cv2.imread(INPUT_DIR_MASK + file_name_temp + '.labels.tif', 0)\n",
    "    \n",
    "    mask_pred = cv2.imread(INPUT_DIR_PATCH_MASK_PRED + file_name_temp + '.jpg', 0)\n",
    "\n",
    "    img_cropped = cv2.resize(img, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "    mask = np.where(mask > 0, 255, 0).astype(np.uint8)\n",
    "    mask_cropped = cv2.resize(mask, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    mask_cropped = np.where(mask_cropped > 128, 255, 0).astype(np.uint8)\n",
    "\n",
    "    mask_pred_cropped = cv2.resize(mask_pred, (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n",
    "    img_gray_mask = np.where(img_gray > 10, 1, 0)\n",
    "\n",
    "    circle = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "    cv2.circle(circle, (int(IMG_SIZE/2), int(IMG_SIZE/2)), int(IMG_SIZE/2), 1, thickness=-1)\n",
    "\n",
    "    img_gray_mask = img_gray_mask*circle\n",
    "\n",
    "    temp = np.zeros((IMG_SIZE, IMG_SIZE), dtype=np.uint8)\n",
    "\n",
    "    for i in range(0, IMG_SIZE-PATCH_SIZE, 100):\n",
    "        for j in range(0, IMG_SIZE-PATCH_SIZE, 100):\n",
    "            if img_gray_mask[i:i+PATCH_SIZE, j:j+PATCH_SIZE].sum() > PATCH_SIZE*PATCH_SIZE/2 and mask_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE].sum() > 0:\n",
    "                img_patch = img_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE,:]\n",
    "                mask_patch = mask_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
    "                mask_pred_cropped_patch = mask_pred_cropped[i:i+PATCH_SIZE, j:j+PATCH_SIZE]\n",
    "                cv2.imwrite(OUTPUT_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.jpg',img_patch)\n",
    "                cv2.imwrite(OUTPUT_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.labels.tif',mask_patch)\n",
    "                cv2.imwrite(OUTPUT_MAKS_DIR + file_name_temp + '_' + str(i) + '_' + str(j) + '.jpg',mask_pred_cropped_patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
