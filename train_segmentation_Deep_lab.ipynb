{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,2'\n",
    "\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import pickle\n",
    "from PIL import ImageFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unet import UNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n",
    "    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose,\n",
    "    RandomCrop, Normalize, Resize\n",
    ")\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def show(img):\n",
    "    npimg = img.detach().numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DeepLabv3 Model download and change the head for your prediction\"\"\"\n",
    "from torchvision import models\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "def createDeepLabv3(outputchannels=1):\n",
    "    model = models.segmentation.deeplabv3_resnet101(\n",
    "        pretrained=True, progress=True)\n",
    "    # Added a Sigmoid activation after the last convolution layer\n",
    "    model.classifier = DeepLabHead(2048, outputchannels)\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: you should change the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "niter = 20\n",
    "outf = './results/prediction_whole_image/'\n",
    "\n",
    "PATH_DATA = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full/'\n",
    "PATH_MASK = '/home/quang/working/fundus_segmentation/data/segmentation_doctor/data_full/'\n",
    "\n",
    "img_size = 512\n",
    "IMG_HEIGHT, IMG_WIDTH = 512, 512\n",
    "batchSize = 4\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# IS_TRAINING = True\n",
    "# n_channel = 3\n",
    "# n_disc = 16\n",
    "# n_gen = 64\n",
    "# n_encode = 64\n",
    "# n_l = 10\n",
    "# n_z = 50\n",
    "# n_age = int(n_z/n_l)\n",
    "# n_gender = int(n_z/2)\n",
    "# n_class_age = 6\n",
    "# n_repeat = 2\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "775 697\n"
     ]
    }
   ],
   "source": [
    "list_paths_img = glob.glob(PATH_DATA + '*.jpg')\n",
    "list_paths_mask = [PATH_MASK + x.split('/')[-1].split('.')[0] + '.labels.tif' for x in list_paths_img]\n",
    "\n",
    "list_paths_img_train, list_paths_img_test, list_paths_mask_train, list_paths_mask_test = train_test_split(list_paths_img, list_paths_mask, \n",
    "                                                                                                          test_size=0.1, random_state=12)\n",
    "print (len(list_paths_img), len(list_paths_img_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAug:\n",
    "    def __init__(self, aug):\n",
    "        self.aug=aug\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.aug(image=img)['image']\n",
    "        return img\n",
    "\n",
    "class FundusDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, PATH_IMG, PATH_MASK, transform=None, transform_torch=None, toTensor=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.PATH_IMG = PATH_IMG\n",
    "        self.PATH_MASK = PATH_MASK\n",
    "        self.ToTensor = toTensor\n",
    "        self.transform = transform\n",
    "        self.transform_torch = transform_torch\n",
    "\n",
    "    def __len__(self):\n",
    "#         return 100\n",
    "        return len(self.PATH_IMG)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name_temp = self.PATH_IMG[idx].split('/')[-1]\n",
    "        image = cv2.imread(self.PATH_IMG[idx])\n",
    "        mask = cv2.imread(self.PATH_MASK[idx],0)\n",
    "        mask = np.where(mask > 0, 1., 0.)\n",
    "        mask = np.expand_dims(mask, axis=-1)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transform:\n",
    "            img_auged = self.transform(image=image, mask=mask)\n",
    "            image = img_auged['image']\n",
    "            mask = img_auged['mask']\n",
    "            image = self.transform_torch(image)\n",
    "            mask = self.ToTensor(mask)\n",
    "        sample = {'image': image, 'mask': mask.float(), 'id': file_name_temp}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    \n",
    "train_album = Compose([\n",
    "    Resize(int(IMG_HEIGHT*1.), int(IMG_WIDTH*1.)), \n",
    "    RandomCrop(IMG_HEIGHT, IMG_HEIGHT),\n",
    "    HorizontalFlip(),\n",
    "    ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=.75)\n",
    "], additional_targets = {'image0': 'image'})\n",
    "\n",
    "train_transform_torch = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "test_album = Compose([\n",
    "    Resize(IMG_HEIGHT, IMG_WIDTH)\n",
    "])\n",
    "\n",
    "test_transform_torch = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "\n",
    "train_dataset = FundusDataset(list_paths_img_train, list_paths_mask_train, transform=train_album, transform_torch=train_transform_torch, \n",
    "                              toTensor=transforms.ToTensor())\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=batchSize, shuffle=True,\n",
    "                                             num_workers=2, drop_last = True)\n",
    "\n",
    "valid_dataset = FundusDataset(list_paths_img_test, list_paths_mask_test, transform=test_album, transform_torch=test_transform_torch, \n",
    "                              toTensor=transforms.ToTensor())\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                             batch_size=1, shuffle=False,\n",
    "                                             num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requires_grad(model, flag=True):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    BCE = nn.BCELoss().cuda()\n",
    "    L1  = nn.L1Loss().cuda()\n",
    "    CE = nn.CrossEntropyLoss().cuda()\n",
    "    MSE = nn.MSELoss().cuda()\n",
    "else:\n",
    "    BCE = nn.BCELoss()\n",
    "    L1  = nn.L1Loss()\n",
    "    CE = nn.CrossEntropyLoss()\n",
    "    MSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "def dice_loss_np(input, target):\n",
    "    smooth = 1.\n",
    "\n",
    "    iflat = input.reshape(-1)\n",
    "    tflat = target.reshape(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    \n",
    "    return 1 - ((2. * intersection + smooth) /\n",
    "              (iflat.sum() + tflat.sum() + smooth))\n",
    "\n",
    "# Focal Tversky loss, brought to you by:  https://github.com/nabsabraham/focal-tversky-unet\n",
    "def tversky(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_pos = y_true.view(-1)\n",
    "    y_pred_pos = y_pred.view(-1)\n",
    "    true_pos = (y_true_pos * y_pred_pos).sum()\n",
    "    false_neg = (y_true_pos * (1-y_pred_pos)).sum()\n",
    "    false_pos = ((1-y_true_pos)*y_pred_pos).sum()\n",
    "    alpha = 0.75\n",
    "    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n",
    "\n",
    "def tversky_loss(y_true, y_pred):\n",
    "    return 1 - tversky(y_true,y_pred)\n",
    "\n",
    "def focal_tversky_loss(y_true,y_pred):\n",
    "    pt_1 = tversky(y_true, y_pred)\n",
    "    gamma = 0.75\n",
    "#     return tf.keras.backend.pow((1-pt_1), gamma)\n",
    "    return (1-pt_1).pow(gamma)\n",
    "\n",
    "focal_loss = FocalLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {'Train': dataloader, 'Test': dataloader_valid}\n",
    "\n",
    "# model = createDeepLabv3()\n",
    "model = nn.DataParallel(createDeepLabv3()).cuda()\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning model\n",
    "\n",
    "### You can weight of BCEWithLogitsLoss with your dataset or change loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/174 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–‰         | 16/174 [00:37<05:47,  2.20s/it]"
     ]
    }
   ],
   "source": [
    "best_loss = 1e10\n",
    "batchsummary = {}\n",
    "niter = 20\n",
    "\n",
    "## Adjust the weight with your dataset\n",
    "weight = torch.tensor([15.])\n",
    "if use_cuda:\n",
    "    weight = weight.cuda()\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "\n",
    "LIST_DICE_VAL = []\n",
    "LIST_BCE_TRAIN = []\n",
    "LIST_DICE_LOSS_TRAIN = []\n",
    "\n",
    "for epoch in range(niter):\n",
    "    print('Epoch {}/{}'.format(epoch, niter))\n",
    "    print('-' * 10)\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_weight = 0\n",
    "    epoch_dice = 0\n",
    "    epoch_bounary = 0\n",
    "    \n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "\n",
    "    for phase in ['Train', 'Test']:\n",
    "        if phase == 'Train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        # Iterate over data.\n",
    "        dice_list = []\n",
    "        sensivity_list = []\n",
    "        specitivity_list = []\n",
    "        for i,sample in enumerate(tqdm(dataloaders[phase])):\n",
    "            inputs = sample['image'].to(device)\n",
    "            masks = sample['mask'].to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'Train'):\n",
    "                outputs = model(inputs)\n",
    "                loss_BCE = criterion(outputs['out'], masks)\n",
    "                \n",
    "                y_pred = torch.sigmoid(outputs['out'])\n",
    "                \n",
    "                masks_probs_flat_tensor = y_pred.view(-1)\n",
    "                true_masks_flat_tensor = masks.view(-1)\n",
    "#                 dice_loss_ = dice_loss(masks_probs_flat_tensor, true_masks_flat_tensor)\n",
    "#                 focal_loss_ = focal_loss(masks_probs_flat_tensor, true_masks_flat_tensor)\n",
    "#                 ft_loss = focal_tversky_loss(true_masks_flat_tensor, masks_probs_flat_tensor)\n",
    "#                 loss = 0.*loss_BCE + dice_loss_  + 0.5*focal_loss_\n",
    "                loss = loss_BCE\n",
    "                           \n",
    "                epoch_loss += loss.mean().item()\n",
    "                y_pred = y_pred.data.cpu().numpy().ravel()\n",
    "                y_true = masks.data.cpu().numpy().ravel()\n",
    "                \n",
    "                y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "                y_true = np.where(y_true > 0.5, 1, 0)\n",
    "                \n",
    "                if phase is not 'Train':\n",
    "                    dice_ = dice_loss_np(y_pred.astype(int), y_true.astype(int))\n",
    "                    dice_list.append(dice_)\n",
    "                else:\n",
    "                    dice_ = 0\n",
    "                    dice_list.append(dice_)\n",
    "                tn_, fp_, fn_, tp_ = confusion_matrix(y_true.reshape(-1).astype(int), y_pred.reshape(-1).astype(int)).ravel()\n",
    "                specificity = tn_ / (tn_+fp_)\n",
    "                sensitivity = tp_ / (tp_+fn_)\n",
    "\n",
    "                sensivity_list.append(sensitivity)\n",
    "                specitivity_list.append(specificity)\n",
    "\n",
    "                if phase == 'Train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "        ## checkpoint\n",
    "        if epoch%10==0 or epoch == (niter-1):\n",
    "            torch.save(model.state_dict(),\"%s/net_%03d.pth\"%(outf,epoch+1))\n",
    "\n",
    "        print('Epoch {} finished ! Loss: {:04.3f} | Loss_weight: {:04.3f} | sensitivity: {:04.3f} | specificity: {}| Dice score: {:04.3f}'.format(epoch,epoch_loss / i, \n",
    "                                                0, np.array(sensivity_list).mean(), np.array(specitivity_list).mean(), 1 - np.array(dice_list).mean())) \n",
    "\n",
    "        print(\"-\"*80)\n",
    "        if  phase is not 'Train':\n",
    "            LIST_DICE_VAL.append(1 - np.array(dice_list).mean())\n",
    "            LIST_BCE_TRAIN.append(epoch_loss / i)\n",
    "            LIST_DICE_LOSS_TRAIN.append(epoch_dice / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
